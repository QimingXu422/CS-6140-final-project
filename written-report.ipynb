{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c336275b386d6439ea67e77e96d9045",
     "grade": false,
     "grade_id": "cell-d21a86d292a3d7ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project Report\n",
    "\n",
    "Write your final project report in the cells below. All written material must be completed prior to your presentation slot. Including figures, appendices, etc. a printed version of this document should be no more than 10 pages and no less than 6 pages. \n",
    "\n",
    "The following cell is used for overall feedback and deductions for length, content, and style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ce7ea5533b1effff2e13157180854ce3",
     "grade": true,
     "grade_id": "cell-343bed4feca7930f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "_(Why would people want to study this dataset and what is the primary task. Find out what the \"target\" variable means and why the customer is interested in running a competition on this dataset.) 2-3 paragraphs_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1b7d89f8e38774cd0b2d59d09f04a82b",
     "grade": true,
     "grade_id": "cell-c9bb94ed861b91bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "This competition is about predicting future payment behavior of clients from application, demographic and historical credit behavior data. Home Credit company provide us this dataset and wanna achieve a machine learning method for future prediction, or determining wether a client should be approved a new loan application. In past, if a new application is submitted, staff in finacial insititution gonna review all information about this application even other information about the client. Like data from external source, and previous loan / application record. But now, with a nice machine learning framwork, we can save a lot of time using the ML model to predict the result in few second instead of letting human review all the documents in hours. \n",
    "\n",
    "The primary task is to build and train a ML model from previous loan data, and use the model to predict the new application should be approved or not. So, it is a binary classification problem. We need a label, which is provided by the training dataset: the \"target\" variable with two possible values: 0 or 1. And since the training dataset provided the target value for each datapoint. It is a superivsed machine learning problem.\n",
    "\n",
    "The reason why customer is intereasted in running a competition on this dataset may varies. Some of them are interesting in loan application process, some want to improve their machine learing skills, some may try to test their new machine learning algorithm with the real world data, or maybe someone just wanna try to win the prize! But overall, the competition do help to the loan application process, saving time for both financial insititution and client. And it can help more people have better financial credit record to achieve their dreams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propsed method\n",
    "_(Present an 1-paragraph description of your method and why you believe it is better that the other things you have tried)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ad319c78971c6e4155af9a1e0aa2cf64",
     "grade": true,
     "grade_id": "cell-6085b28e8af4d158",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The method I propsed is logistic regression with stochastic gradient descent. It is fast have have realatively high AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "_(Find (5-6) examples of people who have worked on similar dataset from the literature. Note: Literature == Published paper in a conference (not stack overflow). Briefly describe in 1-2 sentences the kinds of features, algorithms, or other methods they applied. Also explain why you believe your method is better. Provide a numbered reference id that will appear later in the references section. 3-5 paragraphs_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede8e26b7174b9d514f4cf789af54bde",
     "grade": true,
     "grade_id": "cell-299d7142a9907f1d",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. Credit Risk Assessment using Statistical and Machine Learning: Basic Methodology and Risk Modeling Applications [link](https://link.springer.com/article/10.1023/A:1008699112516) <br>\n",
    "This paper use decision tree, neural network and K-nearest neighbors algorithms to work on the Global Risk Models. \n",
    "\n",
    "2. Support vector machine based multiagent ensemble learning for credit risk evaluation [link](https://www.sciencedirect.com/science/article/pii/S0957417409006617) <br>\n",
    "In this paper, a four-stage support vector machine (SVM) based multiagent ensemble learning approach is proposed for credit risk evaluation. \n",
    "\n",
    "3. Credit risk assessment with a multistage neural network ensemble learning approach [link](https://www.sciencedirect.com/science/article/pii/S0957417407000206) <br>\n",
    "In this study, a multistage neural network ensemble learning model is proposed to evaluate credit risk at the measurement level. \n",
    "\n",
    "4. Neural networks for credit risk evaluation: Investigation of different neural models and learning schemes [link](https://www.sciencedirect.com/science/article/pii/S0957417410001405) <br>\n",
    "This paper describes a credit risk evaluation system that uses supervised neural network models based on the back propagation learning algorithm.\n",
    "\n",
    "5. A new fuzzy support vector machine to evaluate credit risk [link](https://ieeexplore.ieee.org/abstract/document/1556587) <br>\n",
    "This paper use Least Squre SVM to evaluate the credit risk. The LSSVM can transform a quadratic programming problem into a linear programming problem thus reducing the computational complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related implementations\n",
    "_(Find (2-3) examples of what people in Kaggle have done on this particular dataset [[2]](https://www.kaggle.com). Reference the URL of their kernel, post, etc. Describe in 1-2 sentences what they have done and why you think your method is better.) 2-3 paragraphs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b53e3ee2a6b1cb541dd35a70f821025",
     "grade": true,
     "grade_id": "cell-a30ca527d1596437",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n",
    "\n",
    "This is the introduction provided by the home credit. It explained a lot about the dataset so I know what those table and columns mean. And it also provided some introduction to feature selection and basic model.\n",
    "\n",
    "https://www.kaggle.com/nagar500/logistic-regression-75-score\n",
    "\n",
    "Since I use the logistic-regression and want to achieve higher score, I find this post and based on this post tried to inprove my model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "_(Data Analysis: Describe the data analysis you have completed, include 1-2 plots of the most useful features or learnings you have obtained from the dataset. Do not include the code, but do include formulas to anything you have calculated such as different feature combinations, feature selection, or analysis methods. You must use at least one clustering algorithm we have seen in class for an analysis of the data. Provide a link to the specific notebook cell in previous notebooks as a reference.)_ 5-6 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dadf3d2d4ff41cb2e933d964f940c8b4",
     "grade": true,
     "grade_id": "cell-f6715c211519644e",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I start the data analysis based on the part 2, which requires me to find some features that have ig > 0.005 if do split based on it. At first I tried each single feature in just application train table but only ext_source2 and ext_source3 have the ig that satisfy the requirement. \n",
    "\n",
    "Then I tried to join the other table. At first I wanna get all the data from all the tabel. So I left join the whole 7 table in two layer like: \n",
    "(application_train join (previous_application join POS_CASH_balance join instalments_payments join credit_card_balence on sk_id_prev)) join (bureau join bureau_balance on sk_id_bureau) on sk_id_curr), \n",
    "resulting in a extrem long time for extracting data from the database. So, I shrink the dataset size. Just one-layer join the application_train, previous_application, bureau and instalments_payments. In such way the data extraction time is reduced a lot.\n",
    "\n",
    "But only table join is not enough to find 15 features. So I start to create new features. At last step I know the ext_source is good. So I tried to use the multification an sum to create the new feature, called ext_prd and ext_avg. Test prove that this two new features also have ig > 0.005. For information in other table. The easiest way is to find the avg of the varible for each sk_id_curr. So I also extracted some avg value from previous_application, bureau and installments_payments table. And I also picked some other original features from the application_train.\n",
    "\n",
    "With those original and new features, using the same idea as I combine the ext_source. My main job became try to combine those features to generate the new features that have ig > 0.005. I mainly used the add, mutily, subtract and divide operation. And this method is widely used in the Kaggle competition. The rest of work is just trying. Finally I got a series of good features that have ig > 0.005.\n",
    "\n",
    "One thing to mention is that, for those categorical features like name_type_suite and name_housing_type, simple split based on simple feature value cannot get a good ig. But just concatenating their value string to generate new categorical feature is a good way to get higher ig. I think it is because the add operation on string is actually a outer product of those features been combined. And it can give us bigger matrix (smaller division) which grants higher ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methods\n",
    "_(Describe the ML algorithms you used for questions [3.1](part-3.ipynb#Question-3.1), [4.1](part-3.ipynb#Question-4.1), and all others. Focus on the formulas, any feature extractions, parameter tuning, etc. Explain how the algorithm works. E.g., if you used a decision, don't say \"I used a decision tree\", explain briefly how a decision tree works and why it was ideally suited for the dataset you chose.)_ 3-5 paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried two method to solve this probelm: decision tree and logistic regression.\n",
    "\n",
    "At first I tried the decision tree. it is intuitive since in part 2 of the final project I used the information gain to find some features and the information gain is the key point of decision tree. However, during the training I find that the training time for desicion tree is too high, the cost is totally unacceptable. The main reason is that in each split time, for numeric value, the decision tree should test each possible split threshhood, and recaculate the entropy for each sub dataset. Futhermore, I used a sample of size 1000 to train the decision tree, and the auc is not pretty good. Later I tried the logistic regression because its time complexity is far lower than decision tree. And after some test on whole train dataset, the logistic regression proved its accuracy and speed. \n",
    "\n",
    "Then I thought about the SVM, but SVM require caculating the kernal matrix, since the dataset contains more than 15000 rows, the SVM is too costly to be used.\n",
    "\n",
    "Then I tried logistic regression.\n",
    "\n",
    "The logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n",
    "\n",
    "1 / (1 + e^-value)\n",
    "\n",
    "Where e is the base of the natural logarithms (Euler’s number or the EXP() function in your spreadsheet) and value is the actual numerical value that you want to transform. Below is a plot of the numbers between -5 and 5 transformed into the range 0 and 1 using the logistic function.\n",
    "\n",
    "The final result proved that the logistic regression have high AUC among those methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "_(Provide some insights as to why you think that the proposed algorithms and features are good for this dataset. Explain whether you believe these are general properties that might be helpful for similar datasets--what makes them similar and why. What about this dataset made your solution successful. Could we use this for other datasets, if so, what types and why?)_ 3-5 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdb42ea3806602e6aa3782bb2af3f2aa",
     "grade": true,
     "grade_id": "cell-616d5470b36b8952",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "As I said above, I used logistic regression with stochastic gradient descent for question 3.1 and 4.1. The only difference between these two question is the feature selection. \n",
    "\n",
    "Before talking about the logistic regression method, I wanna first talk about the data extracting process. As we know, the data we get from database is not just numeric features but some categorical features. And the SGD method introduced in class can just deal with numbers. So the first step is to \"encode\" those categorical features into numeric.\n",
    "\n",
    "I had two ideas about how to do that. The first one is that for each possible categocial value, create a new feature that have two value 0 and 1, 0 means this datapoint is categorized to this value and 1 means not. And the second idea is that I can transfer the different categorized value into continue integers. For example: Gender have two type male and female. And we can encode male as 1, female as 2, and leave 0 as \"the value of this feature is missing\". Later I decided to use the second method because the first one will increase the feature amound to a big number, which is not good for training. While the second method is easiler to implement.\n",
    "\n",
    "The next step is to deal with the missing value variable. For categorical variable, I use 0 for missing. But it is not a good approach to set 0 for numeric missing value. So I used the mean to fill the missing values.I update the create_dataset method. It will first get raw data from database. And it will check each feature is categorical or numeric. And find the mean for numeric features. Then go through the dataset, pick the id and label field, and transfer categorical value to numeric. During this process, we use a hash to keep record which integer repersenting each categorical value string.Above is for model training. For evaluation, we used the same method. One thing to mention is that for categorical value and missing value, I should keep the training set and test set have same cretierial. So in create_dataset method, besides the data, I also return the numerical_features_mean hash and categorical_features_values hash. The create_test_dataset will take the test dataset as well as these two hash for transefer and missing value filling.\n",
    "\n",
    "Before we using the logistic regression to train the model, we also need to normalize the dataset in order to prevent the fact that the feature with bigger value will impact more on the model. So I used the z_normalize method to normalize the dataset.\n",
    "\n",
    "Now I get a normalized dataset without missing value and categorical value. The rest of work is to train the model using the logistic regression. To reduce the traning time, I used SGD. The SGD works in such way:  \n",
    "\n",
    "Choose an initial vector of parameters w and learning rate lr\n",
    "    Repeat until an approximate minimum is obtained:\n",
    "        Randomly shuffle examples in the training set.\n",
    "            For samle data, do:\n",
    "                update the w by dw and lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ae0fa2d0474dd5cc8c4555d77506c12",
     "grade": true,
     "grade_id": "cell-a5fad0534cafdb6f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "_(Did you use all the data, cross-validation, training / test split, etc? Give enough details on how you setup the experiment so that your colleague can read this section and write their own algorithm to produce the same setup. Provide a link to the cells in the notebooks that contain the experimental setup.)_ 3-4 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7124b6b05c26327c785ea4baa791ee03",
     "grade": true,
     "grade_id": "cell-8e284da64965d4fd",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I used all the data from the application_train dataset, but I didn't use the cross-validtion. \n",
    "\n",
    "If someone want to use their own algorithms on my notebook. He/She only need to write their own algorithms in the cell replace my implementation of the StochasticGradientDescent and LogisticRegressionModelL2. Then they can have a tiny change to the train_xxx_classifier and eval_xxx_classifier and it should works. \n",
    "\n",
    "The part3 code can be found in https://github.com/QimingXu422/CS-6140-final-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "_(Write a table containing the results of your experiments, which were calculated in the notebooks. Include all algorithms included in questions [3.1](part-3.ipynb#Question-3.1) and above in Part 3 of the notebooks. Provide some interpretation of these results. Do you think you could have done better? If so, why did you not pursue those ideas? Add any pictures you think approprate here.)_ 5-6 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "47b0ae14537531217347e46db447bbba",
     "grade": true,
     "grade_id": "cell-384dadfd814ac391",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Question | Features | Algorithms | AUC |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| 2.1 | Features in application_train | Logistic Regression | 0.5876 |\n",
    "| 3.1 | Features in application_train | Logistic Regression | 0.6729 |\n",
    "| 4.1 | Features in application_train | Logistic Regression | 0.7080 |\n",
    "\n",
    "the best result is 4.1, as shown below: \n",
    "![image.png](4.1.png)\n",
    "\n",
    "One thing to mention is that: those result are all using the same model, the only difference is the feature selection:\n",
    "For 2.1, features are: NAME_TYPE_SUITE, EXT_SOURCE_1, ENTRANCES_AVG\n",
    "For 3.1, features are: OWN_CAR_AGE, EXT_SOURCE_1, EXT_SOURCE_2, APARTMENTS_AVG, BASEMENTAREA_AVG, YEARS_BEGINEXPLUATATION_AVG, YEARS_BUILD_AVG, COMMONAREA_AVG, ELEVATORS_AVG, ENTRANCES_AVG\n",
    "For 4.1, features are: EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3, ext_sum and ext_prod\n",
    "\n",
    "We can find that the feature determine the performance a lot.\n",
    "\n",
    "I do think I can do better if I use those feature I extracted from part 2. But since I spent two much time on decision tree I dont have time to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "_(Summarize your findings. If someone wanted to use your solution, which would you recommend? What could you do if you had more data, etc. What should a company seeking to run this at high scale choose if they were to use your method.)_ 3 paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfdbf631dd7ab4a6b983b5f57b05e7d8",
     "grade": true,
     "grade_id": "cell-fac0328f843eb6f5",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The most important thing is that, the engineer should choose appropriate features. Feature engineering is the key point for better performance. And if anyone want use my solution, I would recommend him/her to find and try more features from different tables. \n",
    "\n",
    "And the high score solution in kaggle competion is using the LGBM. After comparision I think the reason why the logistic regression is raletively low is the missing rate for dataset is high. If we just use missing or mean to fill the data. The error is high.\n",
    "\n",
    "If I had more data, I think I can just ignore those data point with many missing values. It may help improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "_(Add a numbered list of the referenced articles, notebooks, etc. that you cited in the above notebooks. Pay attention that the numbers you used correspond to the list below.)_\n",
    "\n",
    "Consider the MLA or APA style, which should be available in Google Scholar.\n",
    "\n",
    "*Example*\n",
    "\n",
    "[[1]](https://arxiv.org) Bob Smith, John Doe. My amazing method. In _Proceedings of WWW 2018_, Lyon France, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97b044cabd4eb3042df6c03c3b625252",
     "grade": true,
     "grade_id": "cell-b608d1f5b8e2570a",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix\n",
    "\n",
    "Add links to your part 1, part 2, and part 3 notebooks (using absolute links). \n",
    "Add anything else you want to here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "030e49a0552af8832a66591584da9636",
     "grade": true,
     "grade_id": "cell-a941b5ed7957cf69",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "All code can be found here: https://github.com/QimingXu422/CS-6140-final-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
