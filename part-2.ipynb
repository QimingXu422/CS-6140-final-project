{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6140 Assignments\n",
    "\n",
    "**Instructions**\n",
    "1. In each assignment cell, look for the block:\n",
    " ```\n",
    "  #BEGIN YOUR CODE\n",
    "  raise NotImplementedError.new()\n",
    "  #END YOUR CODE\n",
    " ```\n",
    "1. Replace this block with your solution.\n",
    "1. Test your solution by running the cells following your block (indicated by ##TEST##)\n",
    "1. Click the \"Validate\" button above to validate the work.\n",
    "\n",
    "**Notes**\n",
    "* You may add other cells and functions as needed\n",
    "* Keep all code in the same notebook\n",
    "* In order to receive credit, code must \"Validate\" on the JupyterHub server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9045008c32b5d644d863fe7f99e1a58",
     "grade": false,
     "grade_id": "cell-2db1ec9fd61f5d6b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Final Project: Part 2 - Feature Extraction\n",
    "\n",
    "\n",
    "In any practical machine learning problem, the data preparation and feature extraction stages are the most important and time-consuming. The final project exposes you to a real-world dataset. In this part of the final project, you are responsible to creating features that will be meaningful for prediction. Features are evaluated based on Information Gain, which you implemented in [Assignment 2](../assignment-2/assignment-2.ipynb).\n",
    "\n",
    "Here is what will work well in this project:\n",
    "\n",
    "* Extract some sample data, load it in [R](https://www.r-project.org), and do some intial analysis. Feel free to build models there to get a feel for the best features.\n",
    "* Join the different tables--they are there for a reason. \n",
    "* Get creative.\n",
    "* Read some of the Kaggle competition forums and kernels. \n",
    "\n",
    "Here is what will NOT work:\n",
    "\n",
    "* Do not use only the features as provided in application_train.\n",
    "* Do not try implementing new learning algorithm in order to generate features. If you find something that works, investigate what features were helpful and add the features. \n",
    "* Do not build lookup tables \"embeddings\" or other things you might have read about but were not covered in class. \n",
    "* Do not try to build a kernel matrix on all pairs. Re-evaluate the kernel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ca0d151ba43bb4deb857d7f8c49f505",
     "grade": false,
     "grade_id": "cell-af1d85683fc29192",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require './assignment_lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79a9a43fb01aab13fbfdc805c3c72da2",
     "grade": false,
     "grade_id": "cell-99c6b85b81c0c166",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<SQLite3::Database:0x000000000bba1ca0 @tracefunc=nil, @authorizer=nil, @encoding=nil, @busy_handler=nil, @collations={}, @functions={}, @results_as_hash=true, @type_translation=nil, @readonly=true>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/home/dataset\"\n",
    "$dev_db = SQLite3::Database.new \"#{dir}/credit_risk_data_dev.db\", results_as_hash: true, readonly: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (10 Points)\n",
    "\n",
    "Implement ```create_dataset``` which runs an SQL query on a database and constructs a dataset like those we have used in this course. Add an ```id``` field for the ```SK_ID_CURR``` and store the ```TARGET``` in ```label```. \n",
    "\n",
    "If the query is:\n",
    "```sql\n",
    "select sk_id_curr, target, ext_source_1 from application_train  where ext_source_1 <> '' order by sk_id_curr;\n",
    "```\n",
    "\n",
    "then the result is:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"label\":1,\"id\":100002,\"features\":{\"ext_source_1\":0.08303696739132256}},\n",
    "    {\"label\":0,\"id\":100015,\"features\":{\"ext_source_1\":0.7220444501416448}}\n",
    "]\n",
    "...\n",
    "```\n",
    "Note the features should not include the ID or Label. Feature keys should be lowercase and only contain keys fo which ```key.is_a? String``` returns true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ad691ec475c4e6aa27160dbd7794be6",
     "grade": false,
     "grade_id": "cell-0a24a7c1d58b392f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":create_dataset"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset db, sql\n",
    "  dataset = []\n",
    "  db.execute sql do |row|\n",
    "    # BEGIN YOUR CODE\n",
    "#     puts row\n",
    "    data_point = Hash.new\n",
    "    data_point[\"features\"] = Hash.new\n",
    "    row.keys.each do |key|\n",
    "      if key.is_a? String\n",
    "        if key == \"SK_ID_CURR\"\n",
    "          data_point[\"id\"] = row[key]\n",
    "        elsif key == \"TARGET\"\n",
    "          data_point[\"label\"] = row[key]\n",
    "        else\n",
    "          data_point[\"features\"][key.downcase] = row[key]\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "    dataset << data_point\n",
    "    #END YOUR CODE\n",
    "  end\n",
    "#   puts dataset[0]\n",
    "  return dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc808c2af502b0b9896f6aa8f645f05d",
     "grade": true,
     "grade_id": "cell-8688ed35b3c26d22",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_11()\n",
    "  dataset = create_dataset $dev_db, \"select sk_id_curr, target, ext_source_1 from application_train where ext_source_1 <> '' \n",
    "order by sk_id_curr limit 37\"\n",
    "  assert_equal 37, dataset.size\n",
    "  assert_true(dataset[0][\"features\"].has_key? \"ext_source_1\")\n",
    "  assert_equal(1, dataset[0][\"features\"].size)\n",
    "  assert_equal(100002, dataset[0][\"id\"])  \n",
    "  assert_in_delta(0.08303696, dataset[0][\"features\"][\"ext_source_1\"], 1e-4)\n",
    "  assert_equal(1, dataset[0][\"label\"])    \n",
    "end\n",
    "\n",
    "test_11()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 (20 points)\n",
    "\n",
    "Copy and revise **your** information gain calculation for numeric and categorical features, from [Assignment 2](../assignment-2/assignment-2.ipynb). Copy the following implementations\n",
    "\n",
    "* Class Distribution\n",
    "* Entropy\n",
    "* Information Gain after splitting\n",
    "* Information gain for numerical features (fast version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75bc54085b65d0a1164a7d8d030f55e6",
     "grade": false,
     "grade_id": "cell-b4fe1101959cb84a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":class_distribution"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_distribution dataset\n",
    "  # BEGIN YOUR CODE\n",
    "  res = Hash.new {|h,k| h[k] = 0}\n",
    "  group = dataset.group_by{|p| p[\"label\"]}\n",
    "  group.keys.each do |k|\n",
    "    res[k] = group[k].length * 1.0 / dataset.length\n",
    "  end\n",
    "  return res\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4194853bca117dcb4696ac0ba1f37872",
     "grade": false,
     "grade_id": "cell-a956fb8dfa56395e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":entropy"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy dist\n",
    "  # BEGIN YOUR CODE\n",
    "  entropy = 0\n",
    "  sum = 0\n",
    "  dist.keys.each do |k|\n",
    "    sum += dist[k]\n",
    "  end\n",
    "  dist.keys.each do |k|\n",
    "    pro = dist[k] * 1.0 / sum\n",
    "    if pro != 0\n",
    "      entropy -= pro * Math.log(pro)\n",
    "    end\n",
    "  end\n",
    "  return entropy\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b58a0a60ccd14c67c0e858cec1e2fe53",
     "grade": true,
     "grade_id": "cell-fe1d965fe70ab0a7",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_1()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  assert_in_delta(0.2686201883261589, h0, 1e-3)\n",
    "end\n",
    "\n",
    "test_12_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba92a5dee595dfa52d2ab80cb6ac9caa",
     "grade": false,
     "grade_id": "cell-9b94563fe65d0166",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":information_gain"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def information_gain h0, splits\n",
    "  # BEGIN YOUR CODE\n",
    "  ig = h0\n",
    "  count = 0\n",
    "  splits.keys.each do |k|\n",
    "    count += splits[k].length\n",
    "  end\n",
    "  splits.keys.each do |k|\n",
    "    subset_entropy = entropy(class_distribution(splits[k]))\n",
    "    ig -= splits[k].length * subset_entropy / count\n",
    "  end\n",
    "  return ig\n",
    "  #END YOUR CODE\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7422895df5de3984725cbc2d5aa2a276",
     "grade": true,
     "grade_id": "cell-d88cded139a1edfd",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_2()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  \n",
    "  splits = dataset.group_by {|row| row[\"features\"][\"flag_own_car\"]}\n",
    "  ig = information_gain h0, splits\n",
    "  assert_in_delta(0.0002206258541794237, ig, 1e-4)\n",
    "end\n",
    "\n",
    "test_12_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "868e0a218c238e6c5c0e2a5fe4750a4c",
     "grade": false,
     "grade_id": "cell-05b14b2b932b0030",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":find_split_point_numeric"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_split_point_numeric y, h0, fname\n",
    "  ### BEGIN SOLUTION\n",
    "  ig_max = 0\n",
    "  t_max = nil\n",
    "  \n",
    "  x = []\n",
    "  y.each do |r|\n",
    "    if r[\"features\"][fname] != \"\"\n",
    "      x << r\n",
    "    end\n",
    "  end\n",
    "\n",
    "  feature_groups = x.group_by {|r| r[\"features\"].fetch(fname, 0.0)}\n",
    "  counts_right = Hash.new {|h,k| h[k] = 0}\n",
    "  counts_left = Hash.new {|h,k| h[k] = 0}\n",
    "  v_left = 0.0\n",
    "  v_right = x.size.to_f\n",
    "\n",
    "  feature_groups.each_key do |t|\n",
    "    counts = Hash.new {|h,k| h[k] = 0}  \n",
    "    feature_groups[t].each do |r| \n",
    "      counts[r[\"label\"]] += 1\n",
    "      counts_right[r[\"label\"]] += 1\n",
    "    end\n",
    "    feature_groups[t] = counts\n",
    "  end\n",
    "  \n",
    "  thresholds = feature_groups.keys.sort\n",
    "  t = thresholds.shift\n",
    "  \n",
    "  feature_groups[t].each_key do |k| \n",
    "    counts_left[k] += feature_groups[t][k]\n",
    "    counts_right[k] -= feature_groups[t][k]\n",
    "    v_left += feature_groups[t][k]\n",
    "    v_right -= feature_groups[t][k]\n",
    "  end\n",
    "  \n",
    "  thresholds.each.with_index do |t, i|\n",
    "    p_left = v_left / x.size\n",
    "    p_right = v_right / x.size\n",
    "    \n",
    "    d_left = Hash.new\n",
    "    d_right = Hash.new\n",
    "    counts_left.each_key {|k| d_left[k] = counts_left[k] / v_left}\n",
    "    counts_right.each_key {|k| d_right[k] = counts_right[k] / v_right}\n",
    "        \n",
    "    h_left = entropy(d_left)\n",
    "    h_right = entropy(d_right)    \n",
    "    ig = h0 - (p_left * h_left + p_right * h_right)\n",
    "    if ig > ig_max\n",
    "      ig_max = ig\n",
    "      t_max = t\n",
    "    end\n",
    "\n",
    "    feature_groups[t].each_key do |k| \n",
    "      counts_left[k] += feature_groups[t][k]\n",
    "      counts_right[k] -= feature_groups[t][k]\n",
    "      v_left += feature_groups[t][k]\n",
    "      v_right -= feature_groups[t][k]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  return [t_max, ig_max]\n",
    "  ### END SOLUTION\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef61a807425ade33886ce44c836d1206",
     "grade": true,
     "grade_id": "cell-8d49eda92182a4b9",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_12_3()\n",
    "  # Check that there are three classes\n",
    "  dataset = create_dataset $dev_db, \"select target, sk_id_curr, ext_source_1, flag_own_car from application_train where ext_source_1 <> ''\"\n",
    "  dist = class_distribution dataset\n",
    "  h0 = entropy dist\n",
    "  \n",
    "  t, ig = find_split_point_numeric dataset, h0, \"ext_source_1\"\n",
    "  assert_in_delta(0.009751743140812785, ig, 1e-4)\n",
    "end\n",
    "\n",
    "test_12_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 (70 Points)\n",
    "\n",
    "Using whatever external software you want (hosted on your own devices), provide 15+ different features that have information >= 0.005. You may to implement several cells below, so please insert them above the test. \n",
    "\n",
    "Features must only be derived from the database but you are free to write whatever SQL queries you want. You may create temporary tables, but the database is read-only.\n",
    "\n",
    "Pay close attention to the following aspects of feature design:\n",
    "\n",
    "* Normalization: Z-score, L2, Min-Max, etc.\n",
    "* Sparsity / missing values\n",
    "* Frequency: Information is easily fooled by features with many values.\n",
    "* Joins: Some of the best features in this dataset combine two columns from different tables.\n",
    "* Transformations: One-hot, Binning, Discretization, Non-linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add extra cells as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":is_number?"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Object\n",
    "  def is_number?\n",
    "    to_f.to_s == to_s || to_i.to_s == to_s\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "60cdc4cc43071b459fbe306658451e3e",
     "grade": false,
     "grade_id": "cell-81b7747d7b0f0592",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":fill_mean"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fill_mean dataset\n",
    "  feature_means = Hash.new{|h,k| h[k] = 0.0}\n",
    "  feature_counts = Hash.new{|h,k| h[k] = 0.0}\n",
    "  feature_sums = Hash.new{|h,k| h[k] = 0.0}\n",
    "  dataset.each do |p|\n",
    "    p[\"features\"].keys.each do |k|\n",
    "      if p[\"features\"][k] != \"\" && p[\"features\"][k].is_number?\n",
    "        feature_counts[k] += 1\n",
    "        feature_sums[k] += p[\"features\"][k]\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  feature_counts.keys.each do |k|\n",
    "    feature_means[k] = feature_sums[k] / feature_counts[k]\n",
    "  end\n",
    "  \n",
    "  dataset.each do |p|\n",
    "    p[\"features\"].keys.each do |k|\n",
    "      if p[\"features\"][k] == \"\"\n",
    "        p[\"features\"][k] = feature_means[k]\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":stdev"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean x\n",
    "  sum = 0\n",
    "  cnt = 0\n",
    "  x.each do |value|\n",
    "    if value != \"\"\n",
    "      sum += value\n",
    "      cnt += 1\n",
    "    end\n",
    "  end\n",
    "  sum / cnt\n",
    "end\n",
    "\n",
    "def stdev x\n",
    "  m = mean x\n",
    "  sum = 0\n",
    "  cnt = 0\n",
    "  x.each do |value|\n",
    "    if value != \"\"\n",
    "      sum += (value - m) ** 2.0\n",
    "      cnt += 1\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  Math.sqrt(sum / cnt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":create_zdatabase"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_zdatabase database\n",
    "  zdatabase = database.clone\n",
    "  zdatabase = database.collect do |r|\n",
    "    u = r.clone\n",
    "    u[\"features\"] = r[\"features\"].clone\n",
    "    u\n",
    "  end\n",
    "  \n",
    "  zdatabase_features = []\n",
    "  zdatabase[0][\"features\"].each_key do |key|\n",
    "    zdatabase_features << key\n",
    "  end\n",
    "\n",
    "  # BEGIN YOUR CODE\n",
    "  means = Hash.new\n",
    "  stdevs = Hash.new\n",
    "  zdatabase_features.each do |f|\n",
    "    if zdatabase.any?{|p| !p[\"features\"][f].is_number?}\n",
    "      next\n",
    "    end\n",
    "    rs = zdatabase.select {|r| r[\"features\"].has_key? f}\n",
    "    x = rs.collect {|r| r[\"features\"][f]} \n",
    "    means[f] = mean x\n",
    "    stdevs[f] = stdev x    \n",
    "    rs.each {|r| r[\"features\"][f] = r[\"features\"][f] == \"\" ? 0 : ((r[\"features\"][f] - means[f]) / stdevs[f])}\n",
    "  end\n",
    "  #END YOUR CODE\n",
    "  return zdatabase\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":extract_features"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features db\n",
    "  dataset = create_dataset $dev_db, \"SELECT\n",
    "  application_train.sk_id_curr,\n",
    "  application_train.target,\n",
    "  application_train.ext_source_1,\n",
    "  application_train.ext_source_2,\n",
    "  application_train.ext_source_3,\n",
    "  application_train.flag_own_car,\n",
    "  application_train.flag_own_realty,\n",
    "  application_train.cnt_children,\n",
    "  application_train.name_contract_type,\n",
    "  application_train.code_gender,\n",
    "  application_train.name_type_suite,\n",
    "  application_train.name_income_type,\n",
    "  application_train.name_education_type,\n",
    "  application_train.name_family_status,\n",
    "  application_train.name_housing_type,\n",
    "  application_train.occupation_type,\n",
    "  application_train.AMT_ANNUITY,\n",
    "  application_train.AMT_GOODS_PRICE,\n",
    "  application_train.AMT_CREDIT,\n",
    "  application_train.DAYS_BIRTH,\n",
    "  application_train.DAYS_EMPLOYED,\n",
    "  application_train.DAYS_ID_PUBLISH,\n",
    "  AVG(previous_application.AMT_ANNUITY) AS prev_amt_annuity,\n",
    "  AVG(previous_application.AMT_CREDIT) AS prev_amt_credit,\n",
    "  AVG(previous_application.AMT_GOODS_PRICE) AS prev_amt_goods_price,\n",
    "  AVG(bureau.AMT_ANNUITY) AS bu_amt_annuity,\n",
    "  AVG(bureau.AMT_CREDIT_SUM_DEBT) AS amt_credit_sum_debt,\n",
    "  AVG(bureau.DAYS_CREDIT) AS days_credit,\n",
    "  AVG(bureau.AMT_CREDIT_SUM) AS amt_credit_sum,\n",
    "  AVG(installments_payments.AMT_PAYMENT) AS amt_payment,\n",
    "  AVG(installments_payments.DAYS_INSTALMENT) AS days_instalment\n",
    "  FROM\n",
    "  application_train\n",
    "  LEFT JOIN bureau ON application_train.sk_id_curr = bureau.sk_id_curr\n",
    "  LEFT JOIN previous_application ON application_train.sk_id_curr = previous_application.sk_id_curr\n",
    "  LEFT JOIN installments_payments ON previous_application.sk_id_prev = installments_payments.sk_id_prev\n",
    "  GROUP BY\n",
    "  application_train.sk_id_curr\"\n",
    "  dataset = fill_mean dataset\n",
    "  splitset = Array.new\n",
    "  \n",
    "  dataset.each do |row|\n",
    "    h = Hash.new\n",
    "    h[\"features\"] = Hash.new\n",
    "    h[\"id\"] = row[\"id\"].clone\n",
    "    h[\"label\"] = row[\"label\"].clone\n",
    "    \n",
    "    h[\"features\"][\"ext_source_2\"] = row[\"features\"][\"ext_source_2\"]\n",
    "    h[\"features\"][\"ext_source_3\"] = row[\"features\"][\"ext_source_3\"]\n",
    "    h[\"features\"][\"ext_prd\"] = \n",
    "    row[\"features\"][\"ext_source_1\"] * row[\"features\"][\"ext_source_2\"] * row[\"features\"][\"ext_source_3\"]\n",
    "    h[\"features\"][\"ext_avg\"] = \n",
    "    (row[\"features\"][\"ext_source_1\"] + row[\"features\"][\"ext_source_2\"] + row[\"features\"][\"ext_source_3\"]) / 3.0\n",
    "    \n",
    "    h[\"features\"][\"f_1\"] = \n",
    "    row[\"features\"][\"flag_own_car\"].to_s + row[\"features\"][\"flag_own_realty\"].to_s + \n",
    "    row[\"features\"][\"name_income_type\"].to_s + row[\"features\"][\"name_housing_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"f_2\"] = \n",
    "    row[\"features\"][\"code_gender\"].to_s + row[\"features\"][\"cnt_children\"].to_s + \n",
    "    row[\"features\"][\"name_education_type\"].to_s + row[\"features\"][\"name_family_status\"].to_s\n",
    "\n",
    "    h[\"features\"][\"f_3\"] = \n",
    "    row[\"features\"][\"name_type_suite\"].to_s + row[\"features\"][\"name_income_type\"].to_s +\n",
    "    row[\"features\"][\"name_housing_type\"].to_s + row[\"features\"][\"name_contract_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"f_4\"] = \n",
    "    row[\"features\"][\"occupation_type\"].to_s + row[\"features\"][\"name_education_type\"].to_s + \n",
    "    row[\"features\"][\"name_housing_type\"].to_s\n",
    "    \n",
    "    h[\"features\"][\"days_compound\"] = \n",
    "    (row[\"features\"][\"days_birth\"].to_f)**2 * (row[\"features\"][\"days_employed\"].to_f)**2 * \n",
    "    (row[\"features\"][\"days_credit\"].to_f * row[\"features\"][\"days_id_publish\"].to_f) / 10**15\n",
    "    \n",
    "    h[\"features\"][\"days_birth_ext\"] = \n",
    "    (row[\"features\"][\"days_birth\"].to_f)**2 * \n",
    "    row[\"features\"][\"ext_source_2\"].to_f / 10**5\n",
    "    \n",
    "    h[\"features\"][\"days_credit_ext\"] = \n",
    "    (row[\"features\"][\"days_credit\"].to_f)**2 * \n",
    "    row[\"features\"][\"ext_source_2\"].to_f / 10**5\n",
    "    \n",
    "    amt_annuity = row[\"features\"][\"amt_annuity\"].to_f\n",
    "    amt_credit = row[\"features\"][\"amt_credit\"].to_f\n",
    "    bu_amt_annuity = row[\"features\"][\"bu_amt_annuity\"].to_f\n",
    "    bu_amt_credit = row[\"features\"][\"amt_credit_sum\"].to_f\n",
    "    amt_credit_sum_debt = row[\"features\"][\"amt_credit_sum_debt\"].to_f\n",
    "    amt_credit_sum = row[\"features\"][\"amt_credit_sum\"].to_f\n",
    "    amt_goods_price = row[\"features\"][\"amt_goods_price\"].to_f\n",
    "    prev_amt_credit = row[\"features\"][\"prev_amt_credit\"].to_f\n",
    "    prev_amt_goods_price = row[\"features\"][\"prev_amt_goods_price\"].to_f\n",
    "    amt_payment = row[\"features\"][\"amt_payment\"].to_f\n",
    "    days_instalment = row[\"features\"][\"days_instalment\"].to_f\n",
    "    \n",
    "    credit_to_annuity = 0.0\n",
    "    if amt_annuity == 0.0\n",
    "      credit_to_annuity = amt_credit\n",
    "    else\n",
    "      credit_to_annuity = amt_credit / amt_annuity\n",
    "    end\n",
    "    \n",
    "    bu_credit_to_annuity = 0.0\n",
    "    if bu_amt_annuity == 0.0\n",
    "      bu_credit_to_annuity = bu_amt_credit\n",
    "    else\n",
    "      bu_credit_to_annuity = bu_amt_credit / bu_amt_annuity\n",
    "    end\n",
    "    \n",
    "    if amt_credit_sum == 0.0\n",
    "      h[\"features\"][\"debt_to_credit\"] = (amt_credit_sum_debt**3) + h[\"features\"][\"ext_source_2\"]\n",
    "    else\n",
    "      h[\"features\"][\"debt_to_credit\"] = \n",
    "      ((amt_credit_sum_debt / amt_credit_sum)**3) + h[\"features\"][\"ext_source_2\"]\n",
    "    end\n",
    "    \n",
    "    credit_minus_price = amt_credit - amt_goods_price\n",
    "    prev_credit_minus_price = prev_amt_credit - prev_amt_goods_price\n",
    "    h[\"features\"][\"credit_minus_price_ext\"] = \n",
    "    (((credit_minus_price + prev_credit_minus_price)) / row[\"features\"][\"ext_source_2\"])\n",
    "    \n",
    "    payment_time = (amt_payment * days_instalment)\n",
    "    h[\"features\"][\"payment_time_ext\"] = \n",
    "    (-payment_time * h[\"features\"][\"ext_source_2\"])\n",
    "    \n",
    "    h[\"features\"][\"credit_to_annuity_ext\"] = \n",
    "    (credit_to_annuity * bu_credit_to_annuity * h[\"features\"][\"ext_source_3\"]**3)\n",
    "\n",
    "    splitset << h\n",
    "  end\n",
    "  splitset = create_zdatabase splitset \n",
    "  return splitset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aa9953ab7fd9e04eaed43e1dac0e393",
     "grade": false,
     "grade_id": "cell-8c402c9f742d0779",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"features\"=>{\"ext_source_2\"=>-1.309768177747291, \"ext_source_3\"=>-2.1509738534295213, \"ext_prd\"=>-1.5210519921268584, \"ext_avg\"=>-3.189769240615041, \"f_1\"=>\"NYWorkingHouse / apartment\", \"f_2\"=>\"M0Secondary / secondary specialSingle / not married\", \"f_3\"=>\"UnaccompaniedWorkingHouse / apartmentCash loans\", \"f_4\"=>\"LaborersSecondary / secondary specialHouse / apartment\", \"days_compound\"=>-0.3517662724499399, \"days_birth_ext\"=>-1.2428388551006582, \"days_credit_ext\"=>-0.559826440020761, \"debt_to_credit\"=>-0.009425163382915479, \"credit_minus_price_ext\"=>-0.0238578320005015, \"payment_time_ext\"=>-0.5498353477808251, \"credit_to_annuity_ext\"=>-0.21453089880644846}, \"id\"=>100002, \"label\"=>1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_dataset = extract_features($dev_db)\n",
    "extracted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4597536fdbf10caf3336be5abe46f2c",
     "grade": true,
     "grade_id": "cell-ef86efb741a938de",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_not_nil extracted_dataset\n",
    "assert_equal 15334, extracted_dataset.size\n",
    "assert_true(extracted_dataset.all? {|row| row[\"features\"].size >= 8}, \"At least 6 non-zero features per row\")\n",
    "assert_true(extracted_dataset.flat_map {|row| row[\"features\"].keys}.uniq.size >= 15,  \"At least 15 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe8a6cc82645651810e2e301d81d06c5",
     "grade": true,
     "grade_id": "cell-23647d16738e3205",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal 15334, extracted_dataset.collect {|row| row[\"id\"]}.uniq.size\n",
    "assert_equal 2, extracted_dataset.collect {|row| row[\"label\"]}.uniq.size\n",
    "\n",
    "h0 = entropy(class_distribution(extracted_dataset))\n",
    "assert_in_delta(0.2797684909805576, h0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "773639e034e9c13094d055ea55cc4911",
     "grade": true,
     "grade_id": "cell-fe9232a9bed335ff",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ext_source_2\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extracted_dataset.flat_map {|row| row[\"features\"].keys}.uniq\n",
    "numeric_features = features.select {|k| extracted_dataset.reject {|row| row[\"features\"][k] == \"\"}.all? {|row| row[\"features\"].fetch(k, 0.0).is_a? Numeric}}\n",
    "\n",
    "assert_true(numeric_features.size >= 4, \"At least 4 numeric features\")\n",
    "def test_ig_numeric extracted_dataset, h0, test_feature1\n",
    "  t, ig = find_split_point_numeric extracted_dataset, h0, test_feature1\n",
    "  assert_true(ig >= 0.005, \"Expected information gain for '#{test_feature1}' > 0.005\")\n",
    "  return test_feature1\n",
    "end\n",
    "\n",
    "test_ig_numeric extracted_dataset, h0, numeric_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0cefa2dba0e524d1aaaf262dcf46eeb5",
     "grade": true,
     "grade_id": "cell-e3cfb6234aae3cd6",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ext_source_3\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ig_numeric extracted_dataset, h0, numeric_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43b12cb9172f05d3cfb69345edf5caec",
     "grade": true,
     "grade_id": "cell-3af2ae9935c80d63",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ext_prd\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ig_numeric extracted_dataset, h0, numeric_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02a54a39ba9c41be717488e7b0451902",
     "grade": true,
     "grade_id": "cell-7eb5e45fb12bab22",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.upto(numeric_features.size - 1) do |i|\n",
    "  test_ig_numeric extracted_dataset, h0, numeric_features[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9401c8458fc1db657539eef316489f6",
     "grade": true,
     "grade_id": "cell-433d61446fb448fb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005604922071159194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"f_1\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = features.select {|k| extracted_dataset.all? {|row| row[\"features\"].fetch(k, \"\").is_a? String}}\n",
    "\n",
    "assert_true(categorical_features.size >= 4, \"At least 4 categorical features\")\n",
    "\n",
    "def test_ig_categorical extracted_dataset, h0, test_feature1\n",
    "  splits = extracted_dataset.group_by {|row| row[\"features\"][test_feature1]}\n",
    "  ig = information_gain h0, splits\n",
    "  puts ig\n",
    "  assert_true(ig >= 0.005, \"Expected information gain for '#{test_feature1}' > 0.005\")\n",
    "  return test_feature1\n",
    "end\n",
    "\n",
    "test_ig_categorical extracted_dataset, h0, categorical_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecc1f4e97d6c2a0809f21488f2574195",
     "grade": true,
     "grade_id": "cell-c066b4ac7af283e4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0069960045246263616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"f_2\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ig_categorical extracted_dataset, h0, categorical_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc543b59d6c6c0b4e70e6ea5ae4d1d37",
     "grade": true,
     "grade_id": "cell-cace39d2ef8a94bc",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007391439933385516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"f_3\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ig_categorical extracted_dataset, h0, categorical_features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af050a047b9388cfbe82a35515d10b27",
     "grade": true,
     "grade_id": "cell-c57fca43e0641c05",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010408435930001108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.upto(categorical_features.size - 1) do |i|\n",
    "  test_ig_categorical extracted_dataset, h0, categorical_features[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
